defaults:
  - base_model

name: qwen2_5_0_5b
vocab_size: 151936
output_size: 151936
bos_token_id: 151643
eos_token_id: 151643
num_hidden_layers: 24
hidden_size: 896
num_attention_heads: 14
num_key_value_heads: 2
intermediate_size: 4864
initializer_range: 0.02
rms_norm_eps: 1e-6
tie_word_embeddings: True
rope_theta: 1000000
seq_len: 32768
attention_pattern: full
