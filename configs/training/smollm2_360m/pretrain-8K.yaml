defaults:
  - /training/base_training@_global_.training

dataset_name: dclm_filter_8k
data_split: train
tokenizer_name: HuggingFaceTB/SmolLM2-360M

total_steps: 9600
seq_length: 8192
global_batch_size: 64
ilr_warmup_steps: 960
optimizer_outer:
  optimizer_type: adamw
  b1: 0.9
  b2: 0.95
  clip_gradient: 1.0
  end_lr: 1e-05
  init_lr: 0.0
  lr: 1.5e-03
  lr_decay_steps: 9600
  lr_warmup_steps: 960
  weight_decay: 0.1
