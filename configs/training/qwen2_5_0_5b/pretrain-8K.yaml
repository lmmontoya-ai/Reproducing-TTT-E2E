defaults:
  - /training/base_training@_global_.training

dataset_name: dclm_filter_8k
data_split: train
tokenizer_name: Qwen/Qwen2.5-0.5B

total_steps: 12000
seq_length: 8192
global_batch_size: 64
ilr_warmup_steps: 1200
optimizer_outer:
  optimizer_type: adamw
  b1: 0.9
  b2: 0.95
  clip_gradient: 1.0
  end_lr: 1e-05
  init_lr: 0.0
  lr: 1.2e-03
  lr_decay_steps: 12000
  lr_warmup_steps: 1200
  weight_decay: 0.1
