# @package _global_
defaults:
  - override /training: 760m/ext
  - override /model: 760m
  - _self_

training:
  exp_name: ext-760m-fa-16K
  seq_length: 16384
  global_batch_size: 32

model:
  seq_modeling_block: self_attention
  force_flash: True
  mini_batch_size: 16384
  rope_theta: 1000000
  remat_block: nothing_saveable
  remat_block_bwd: nothing_saveable
