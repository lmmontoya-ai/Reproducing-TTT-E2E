# @package _global_
defaults:
  - override /training: qwen2_5_0_5b/pretrain-8K
  - override /model: qwen2_5_0_5b
  - _self_

training:
  exp_name: adapt-qwen05-e2e-8K-from-import
  load_part: params
  resume_exp_name: adapt-qwen05-swa-8K-from-import
  train_mode: meta
  spec_inner: ["language_model.**.suffix_blocks.feed_forward_prime.**"]
  ilr_init: 0.1
  optimizer_inner:
    optimizer_type: sgd
    lr: 1
    clip_gradient: 1.0
  init_source: external_hf
  external_model_id: Qwen/Qwen2.5-0.5B
  external_profile_path: ./artifacts/external_models/qwen2_5_0_5b/model_profile.json
  adapter_recipe: ttt_bridge

model:
  seq_modeling_block: SWA
  force_flash: False
  unroll_block_scan: True
  mini_batch_size: 1024
  sliding_window_size: 8192
  rope_theta: 1000000
  prime: True
  suffix_len: 6
  feed_forward_prime: swiglu
  attention_pattern: ttt_swa
