# @package _global_
defaults:
  - override /training: smollm2_360m/pretrain-8K
  - override /model: smollm2_360m
  - _self_

training:
  exp_name: adapt-smol360-swa-8K-from-import
  load_part: params
  resume_exp_name: pretrain-smol360-fa-import-8K
  train_mode: pretrain
  init_source: external_hf
  external_model_id: HuggingFaceTB/SmolLM2-360M
  external_profile_path: ./artifacts/external_models/smollm2_360m/model_profile.json
  adapter_recipe: swa_bridge

model:
  seq_modeling_block: SWA
  force_flash: False
  unroll_block_scan: True
  mini_batch_size: 2048
  sliding_window_size: 4096
  rope_theta: 100000
  prime: False
  suffix_len: 0
  attention_pattern: swa
