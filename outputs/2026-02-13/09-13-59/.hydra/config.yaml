backend:
  distributed: false
  coordinator_address: null
  num_processes: 1
  process_id: 0
  local_device_ids: 0,1,2,3,4,5,6,7
  backend: gpu
  num_devices: 8
  compilation_cache_dir: /tmp/jax_cache
model:
  name: qwen2_5_0_5b
  vocab_size: 151936
  hidden_size: 896
  intermediate_size: 4864
  num_hidden_layers: 24
  num_attention_heads: 14
  num_key_value_heads: 2
  mini_batch_size: 2048
  sliding_window_size: 8192
  seq_len: 32768
  rms_norm_eps: 1.0e-06
  initializer_range: 0.02
  bos_token_id: 151643
  eos_token_id: 151643
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  tie_word_embeddings: true
  remat_block: ''
  remat_block_bwd: ''
  remat_prefix_block: nothing_saveable
  remat_attention: ''
  remat_attention_bwd: ''
  remat_mlp: ''
  remat_mlp_bwd: ''
  remat_rms: ''
  remat_rms_bwd: ''
  remat_multiple_gd: ''
  seq_modeling_block: SWA
  rope_theta: 1000000.0
  output_size: 32000
  compute_dtype: bf16
  param_dtype: fp32
  state_dtype: fp32
  unroll_block_scan: true
  unroll_inner_scan: false
  force_flash: false
  suffix_len: 0
  prime: false
  qk_norm: true
  pre_norm: true
  post_norm: true
  feed_forward_prime: swiglu
  attention_pattern: swa
training:
  log_wandb: true
  wandb_entity: phase1
  wandb_project: phase1
  wandb_key: none
  model_seed: 0
  data_seed: 0
  load_part: params
  total_steps: 1
  break_step: -1
  save_milestone_freq: 1
  dataset_path: ${deploy_paths.data[${training.dataset_name}]}
  dataset_name: dclm_filter_8k
  tokenizer_name: meta-llama/Llama-2-7b-hf
  seq_length: 64
  global_batch_size: 2
  accum_steps: 1
  loader_workers: 32
  dummy_dataset: false
  checkpoint_path: /private/tmp/external_ckpt_real
  exp_dir: /private/tmp/external_phase1_real
  exp_folder: external_qwen_adapter_smoke
  exp_name: adapt-qwen05-swa-8K-from-import
  resume_exp_name: pretrain-qwen05-fa-import-8K
  resume_step: null
  init_source: external_hf
  external_model_id: Qwen/Qwen2.5-0.5B
  external_profile_path: /private/tmp/external_profiles_test/qwen2_5_0_5b/model_profile.json
  adapter_recipe: swa_bridge
  eval_mode: false
  train_mode: pretrain
  runtime_mode: token_stats
  data_split: train
  eval_split: val
  inner_remat_freq: 1
  optimizer_outer:
    optimizer_type: adamw
    init_lr: 0.0
    end_lr: 1.0e-05
    lr: 0.0012
    lr_warmup_steps: 1200
    lr_decay_steps: 12000
    b1: 0.9
    b2: 0.95
    clip_gradient: 1.0
    weight_decay: 0.1
    bf16_momentum: false
    emb_wd: true
  optimizer_inner:
    optimizer_type: sgd
    init_lr: ???
    end_lr: ???
    lr: 0.01
    lr_warmup_steps: ???
    lr_decay_steps: ???
    b1: ???
    b2: ???
    clip_gradient: 0.0
    weight_decay: ???
    bf16_momentum: ???
  spec_outer:
  - '**'
  spec_inner:
  - '**'
  n_data_parallel: null
  n_state_parallel: 1
  ilr_warmup_steps: 1200
  ilr_init: 1.0
  eval_batch_size: 8
checkpoint:
  float_dtype: bf16
  save_optimizer_state: true
  checkpoint_dir: ${deploy_paths.checkpoint}/${training.exp_folder}/${training.exp_name}
  resume_checkpoint_dir: ${deploy_paths.checkpoint}/${training.exp_folder}/${training.resume_exp_name}
deploy_paths:
  data:
    dclm_filter_8k: /private/tmp/phase1_token_data
    books3: /private/tmp/phase1_token_data
  checkpoint: /private/tmp/external_ckpt_real
