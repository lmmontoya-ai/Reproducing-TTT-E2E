backend:
  distributed: false
  coordinator_address: null
  num_processes: 1
  process_id: 0
  local_device_ids: 0,1,2,3,4,5,6,7
  backend: gpu
  num_devices: 8
  compilation_cache_dir: /tmp/jax_cache
model:
  name: smollm2_360m
  vocab_size: 49152
  hidden_size: 960
  intermediate_size: 2560
  num_hidden_layers: 32
  num_attention_heads: 15
  num_key_value_heads: 5
  mini_batch_size: 8192
  sliding_window_size: 1024
  seq_len: 32768
  rms_norm_eps: 1.0e-05
  initializer_range: 0.02
  bos_token_id: 0
  eos_token_id: 0
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  tie_word_embeddings: true
  remat_block: nothing_saveable
  remat_block_bwd: nothing_saveable
  remat_prefix_block: nothing_saveable
  remat_attention: ''
  remat_attention_bwd: ''
  remat_mlp: ''
  remat_mlp_bwd: ''
  remat_rms: ''
  remat_rms_bwd: ''
  remat_multiple_gd: ''
  seq_modeling_block: self_attention
  rope_theta: 100000.0
  output_size: 32000
  compute_dtype: bf16
  param_dtype: fp32
  state_dtype: fp32
  unroll_block_scan: false
  unroll_inner_scan: false
  force_flash: true
  suffix_len: 0
  prime: false
  qk_norm: true
  pre_norm: true
  post_norm: true
  feed_forward_prime: swiglu
  attention_pattern: full
training:
  log_wandb: true
  wandb_entity: phase1
  wandb_project: phase1
  wandb_key: none
  model_seed: 0
  data_seed: 0
  load_part: none
  total_steps: 1
  break_step: -1
  save_milestone_freq: 1
  dataset_path: ${deploy_paths.data[${training.dataset_name}]}
  dataset_name: dclm_filter_8k
  tokenizer_name: meta-llama/Llama-2-7b-hf
  seq_length: 8192
  global_batch_size: 64
  accum_steps: 1
  loader_workers: 32
  dummy_dataset: true
  checkpoint_path: /tmp/ext_smoke_checkpoints
  exp_dir: /tmp/ext_smoke_experiments
  exp_folder: external_smoke
  exp_name: pretrain-smol360-fa-scratch-8K
  resume_exp_name: ''
  resume_step: null
  init_source: scratch
  external_model_id: ''
  external_profile_path: ''
  adapter_recipe: none
  eval_mode: false
  train_mode: pretrain
  runtime_mode: simulate
  data_split: train
  eval_split: val
  inner_remat_freq: 1
  optimizer_outer:
    optimizer_type: adamw
    init_lr: 0.0
    end_lr: 1.0e-05
    lr: 0.0015
    lr_warmup_steps: 960
    lr_decay_steps: 9600
    b1: 0.9
    b2: 0.95
    clip_gradient: 1.0
    weight_decay: 0.1
    bf16_momentum: false
    emb_wd: true
  optimizer_inner:
    optimizer_type: sgd
    init_lr: ???
    end_lr: ???
    lr: 0.01
    lr_warmup_steps: ???
    lr_decay_steps: ???
    b1: ???
    b2: ???
    clip_gradient: 0.0
    weight_decay: ???
    bf16_momentum: ???
  spec_outer:
  - '**'
  spec_inner:
  - '**'
  n_data_parallel: null
  n_state_parallel: 1
  ilr_warmup_steps: 960
  ilr_init: 1.0
  eval_batch_size: 8
checkpoint:
  float_dtype: bf16
  save_optimizer_state: true
  checkpoint_dir: ${deploy_paths.checkpoint}/${training.exp_folder}/${training.exp_name}
  resume_checkpoint_dir: ${deploy_paths.checkpoint}/${training.exp_folder}/${training.resume_exp_name}
deploy_paths:
  data:
    dclm_filter_8k: /tmp/phase1_token_data
    books3: /tmp/phase1_token_data
  checkpoint: /tmp/ext_smoke_checkpoints
